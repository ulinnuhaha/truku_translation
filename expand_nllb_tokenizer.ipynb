{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487988cf",
   "metadata": {},
   "source": [
    "## Load the dataset of truku and traditional chinese languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e8ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_data(file):\n",
    "    df=pd.read_excel('./dataset/'+file+'.xlsx')\n",
    "    df =df.replace(to_replace=r'_x000D_', value='', regex=True) #delete _x000D_ found in the element\n",
    "    df=df.rename(columns={\"華語\": \"chinese\", \"太魯閣族語\": \"truku\"}) #rename columns\n",
    "    return df[['chinese','truku']]\n",
    "df1=get_data('df1')\n",
    "df2=get_data('df2')\n",
    "df3=get_data('df3')\n",
    "df4=get_data('df4') #bible\n",
    "df=pd.concat([df1,df2,df3,df4]) # concatinate all data\n",
    "df=df.drop_duplicates() #drop duplicates of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad82e3b",
   "metadata": {},
   "source": [
    "## Checking How well does the data fit into the NLLB tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564117fa",
   "metadata": {},
   "source": [
    "### How many unknown tokens are in the tokenizer outputs for Truku or traditional Chinese?\n",
    "#### If this is too often, we need to fix it somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NLLB Tokenizer\n",
    "from transformers import NllbTokenizer \n",
    "tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae8895e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2bd004f87a4f8ba657349887bc9789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# counting the amount of unknown tokens\n",
    "from tqdm.auto import tqdm\n",
    "# In Truku data\n",
    "unk=[] \n",
    "for sent in tqdm(df['truku']):\n",
    "    for text in tokenizer.tokenize(str(sent)):\n",
    "        if tokenizer.convert_tokens_to_ids(text) == tokenizer.unk_token_id:\n",
    "            unk.append(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c16b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unknown tokens in truku dataset is  58\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "unk_tr_tok=Counter(c for t in unk for c in t)\n",
    "print('The number of unknown tokens in truku dataset is ',len(unk_tr_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c29750f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fcb8fed8054e39ae9b044402e09c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In chinese data\n",
    "unk_zh=[]\n",
    "for sent in tqdm(df['chinese']):\n",
    "    for text in tokenizer.tokenize(str(sent)):\n",
    "        if tokenizer.convert_tokens_to_ids(text) == tokenizer.unk_token_id:\n",
    "            unk_zh.append(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e435ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unknown tokens in chinese dataset is  1488\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "unk_zh_tok=Counter(c for t in unk_zh for c in t)\n",
    "print('The number of unknown tokens in chinese dataset is ',len(unk_zh_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf66d80",
   "metadata": {},
   "source": [
    "### We can also evaluate based on How many tokens per word do we have on average? \n",
    "#### However this scheme can be applied in in alphabetical letter, i.e. Truku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split a text into words, numbers, and punctuation marks\n",
    "    (for languages where words are separated by spaces)\n",
    "    \"\"\"\n",
    "    return re.findall('(\\w+|[^\\w\\s])', text)\n",
    "\n",
    "smpl = df.sample(10000, random_state=1)\n",
    "#smpl['chinese_toks'] = smpl.chinese.apply(tokenizer.tokenize)\n",
    "smpl['truku_toks'] = smpl.truku.apply(tokenizer.tokenize)\n",
    "#smpl['chinese_words'] = smpl.chinese.apply(word_tokenize)\n",
    "smpl['truku_words'] = smpl.truku.apply(word_tokenize)\n",
    "stats = smpl[\n",
    "    [ 'truku_toks', 'truku_words']\n",
    "].applymap(len).describe()\n",
    "#print(stats.chinese_toks['mean'] / stats.chinese_words['mean'])  # 4.0333\n",
    "print(stats.truku_toks['mean'] / stats.truku_words['mean'])  # 1.707\n",
    "print(stats)\n",
    "#Good news: for truku, as a new language, the NLLB tokenizer produces on average 1.7 tokens per word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e40f89",
   "metadata": {},
   "source": [
    "## Increase the vocabs of traditional Chinese in NLLB Tokenizer\n",
    "### Since there are so many unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4e0b0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8517181/.local/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# retrieve traditional Chinese words from the owned dataset \n",
    "# and a new huge dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "zh_tra = load_dataset(\"jed351/Traditional-Chinese-Common-Crawl-Filtered\", data_files=\"C4_Traditional_Chinese-00004-of-00008.jsonl\", split=\"train\")\n",
    "zh_tra = zh_tra.remove_columns([\"url\",\"timestamp\", \"content_language\", \"content_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adffa0",
   "metadata": {},
   "source": [
    "### The codes for preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e1a7347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               #u\"\\U000024C2-\\U0001F251\"# chinese char\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae91ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "from cleantext import clean as Clean\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    #clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(text)\n",
    "    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    \n",
    "    clean = remove_emoji(clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a74fb1",
   "metadata": {},
   "source": [
    "### Put together all texts of traditional Chinese words from all datasets, then preprocess them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52faf16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312908326\n",
      "1964285\n",
      "6261647\n"
     ]
    }
   ],
   "source": [
    "print(sum(len(t) for t in zh_tra['text']))  # 312908326\n",
    "print(sum(len(str(t)) for t in df['chinese'].dropna())) #1964285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf01d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "from collections import Counter\n",
    "zh_all_texts = zh_tra['text'] + df.chinese.dropna().tolist() \n",
    "zh_all_text_normalized = [preproc(t) for t in tqdm(str(zh_all_texts))]\n",
    "zh_chars_cnt = Counter(c for t in zh_all_text_normalized for c in t)\n",
    "# count the number characters appear more 3 times\n",
    "zh_required_chars = ''.join([\n",
    "    k for k, v in zh_chars_cnt.most_common() \n",
    "    if v >= 3 and k not in ' '\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b0b9e",
   "metadata": {},
   "source": [
    "### save the traditional chinese data corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b047c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('zht_all_texts.txt','w')\n",
    "file.writelines(str(zh_all_texts))\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01009c63",
   "metadata": {},
   "source": [
    "### Expanding the vocabulary of traditional chinese language in NLLB Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "all_texts_file = 'zht_all_texts.txt'\n",
    "SPM_PREFIX = 'spm_zh_tr_20k'\n",
    "with open(all_texts_file, 'w') as f:\n",
    "    for i, text in enumerate(zh_all_texts):\n",
    "        print(text, file=f)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=all_texts_file,\n",
    "    model_prefix=SPM_PREFIX,\n",
    "    vocab_size=20*(1000),  # 20K\n",
    "    character_coverage = 1,\n",
    "    num_threads=16,\n",
    "    train_extremely_large_corpus=False,\n",
    "    add_dummy_prefix=False,\n",
    "    max_sentencepiece_length=128,\n",
    "    max_sentence_length=4192*4,\n",
    "    pad_id=0,\n",
    "    eos_id=1,\n",
    "    unk_id=2,\n",
    "    bos_id=-1,\n",
    "    required_chars=zh_required_chars,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a72ee81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "# At this step, the code may throw an error about protobuf. Do as it tells.\n",
    "from transformers import NllbTokenizer\n",
    "SPM_PREFIX = 'spm_zh_tr_20k'\n",
    "# reading the NLLB and the traditional chinese sentencepiece models into a native format\n",
    "tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')\n",
    "sp_trained = spm.SentencePieceProcessor(model_file=f'{SPM_PREFIX}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32946b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExtracting the sentencepiece model from the standard NLLB tokenizer and enriching it from all tokens from new traditional Chinese\\ntokenizer that has been missing from the NLLB tokenizer \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Extracting the sentencepiece model from the standard NLLB tokenizer and enriching it from all tokens from new traditional Chinese\n",
    "tokenizer that has been missing from the NLLB tokenizer \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "278a039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4852054"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "chinese_spm = sp_pb2_model.ModelProto() ## Define the sentencepiece model of traditional Chinese tokenizer\n",
    "chinese_spm.ParseFromString(sp_trained.serialized_model_proto()) # Get the serialization model from traditional Chinese SentencePiece tokenizer and parse it\n",
    "nllb_spm = sp_pb2_model.ModelProto() ## Define the sentencepiece model of NLLB tokenizer\n",
    "nllb_spm.ParseFromString(tokenizer.sp_model.serialized_model_proto())# Load sentencepiece model from NLLB tokenizer and parse it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cba19",
   "metadata": {},
   "source": [
    "### a set of all vocabs in NLLB tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af34fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nllb_tokens_set = {p.piece for p in nllb_spm.pieces}\n",
    "prev_min_score = nllb_spm.pieces[-1].score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1cd2c",
   "metadata": {},
   "source": [
    "### adding the missing tokens of traditional chinese to the NLLB sentencepiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88fe9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_p=0\n",
    "for p in chinese_spm.pieces: \n",
    "    piece = p.piece\n",
    "    if piece not in nllb_tokens_set: #if the token (in chinese spm) not available in NLLB token\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        \n",
    "        # for all new tokens, set a lower score (priority)\n",
    "        new_p.score = p.score + prev_min_score\n",
    "        nllb_spm.pieces.append(new_p)\n",
    "        total_p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e28f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new spm model to directory\n",
    "NEW_SPM_NAME = 'spm_nllb_ch_tr_270k.model'\n",
    "with open(NEW_SPM_NAME, 'wb') as f:\n",
    "    f.write(nllb_spm.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59185cf2",
   "metadata": {},
   "source": [
    "## Update the neural network weights: add new embeddings for the freshly added tokens to NLLB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ab9f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256204 270333\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "# loading the tokenizers\n",
    "tokenizer_old = NllbTokenizer.from_pretrained(model_name)\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file=NEW_SPM_NAME)\n",
    "print(len(tokenizer_old), len(tokenizer)) # 256204, 270333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "beaa4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the default NLLB tokenizer 256204\n",
      "The number of tokens in the new NLLB tokenizer 270333\n"
     ]
    }
   ],
   "source": [
    "print('The number of tokens in the default NLLB tokenizer', len(tokenizer_old))\n",
    "print('The number of tokens in the new NLLB tokenizer', len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "beaabcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of added tokens 14128\n"
     ]
    }
   ],
   "source": [
    "# add new 14128 tokens\n",
    "added_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))\n",
    "print('The number of added tokens', len(added_vocab))  # 14128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa123ddf",
   "metadata": {},
   "source": [
    "### Add truku as a new language tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce3ae3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tokenizer(tokenizer, new_lang='tru_Latn'):\n",
    "    \"\"\" Add a new language token to the tokenizer vocabulary (this should be done each time after its initialization) \"\"\"\n",
    "    old_len = len(tokenizer) - int(new_lang in tokenizer.added_tokens_encoder)\n",
    "    tokenizer.lang_code_to_id[new_lang] = old_len-1\n",
    "    tokenizer.id_to_lang_code[old_len-1] = new_lang\n",
    "    # always move \"mask\" to the last position\n",
    "    tokenizer.fairseq_tokens_to_ids[\"<mask>\"] = len(tokenizer.sp_model) + len(tokenizer.lang_code_to_id) + tokenizer.fairseq_offset\n",
    "\n",
    "    tokenizer.fairseq_tokens_to_ids.update(tokenizer.lang_code_to_id)\n",
    "    tokenizer.fairseq_ids_to_tokens = {v: k for k, v in tokenizer.fairseq_tokens_to_ids.items()}\n",
    "    if new_lang not in tokenizer._additional_special_tokens:\n",
    "        tokenizer._additional_special_tokens.append(new_lang)\n",
    "    # clear the added token encoder; otherwise a new token may end up there by mistake\n",
    "    tokenizer.added_tokens_encoder = {}\n",
    "    tokenizer.added_tokens_decoder = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f78e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d8f6f",
   "metadata": {},
   "source": [
    "### Loading and resizing the NLLB model (from pytorch_model.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b608bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28ec77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256206, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "        [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "        [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "        ...,\n",
       "        [-0.0123, -0.0031, -0.0089,  ...,  0.0645, -0.0182, -0.0740],\n",
       "        [ 0.0085, -0.0088, -0.0091,  ...,  0.0571, -0.0035, -0.1298],\n",
       "        [-0.0076, -0.0107, -0.0051,  ...,  1.0264, -0.0338, -0.1175]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Before extending the tokenizer\n",
    "print(model.model.shared.weight.data.shape)\n",
    "model.model.shared.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ec81a",
   "metadata": {},
   "source": [
    "#### Resize the model embedding shape adjusted with the lenght of extended tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87117cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 270334. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(270334, 1024)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "##The embedding for the new token is by default initialized randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3aea9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([270334, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0321,  0.0348,  0.0181,  ...,  0.0312, -0.0099, -0.0133],\n",
       "        [-0.0039,  0.0104, -0.0156,  ...,  0.0290, -0.0138, -0.0134],\n",
       "        [-0.0245, -0.0283, -0.0295,  ...,  0.9712, -0.0255, -0.0273],\n",
       "        ...,\n",
       "        [ 0.0162,  0.0015,  0.0258,  ...,  0.0093, -0.0138,  0.0087],\n",
       "        [-0.0051, -0.0136,  0.0166,  ..., -0.0254,  0.0095, -0.0316],\n",
       "        [ 0.0427, -0.0335, -0.0056,  ..., -0.0394, -0.0203, -0.0277]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## After extending the tokenizer\n",
    "print(model.model.shared.weight.data.shape)\n",
    "model.model.shared.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa15923",
   "metadata": {},
   "source": [
    "### Set the embedding of lang id sames as the default NLLB tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74061f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "moved_tokens = list(tokenizer_old.lang_code_to_id) + ['<mask>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "634fd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.shared.weight.data[tokenizer.convert_tokens_to_ids(moved_tokens)] = model.model.shared.weight.data[tokenizer_old.convert_tokens_to_ids(moved_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d786223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the truku embedding same with tagalog embedding since both are in same family, i.e., austronesian\n",
    "model.model.shared.weight.data[tokenizer.convert_tokens_to_ids('tru_Latn')] = model.model.shared.weight.data[tokenizer_old.convert_tokens_to_ids('tag_Latn')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1df03",
   "metadata": {},
   "source": [
    "### re-initializing the new embeddings for new vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b77cd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c86f5a7f8a4f99b34d9849b20929da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty token \"‍\"/258688\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "for t in tqdm(added_vocab):\n",
    "    tt = tokenizer_old(t, add_special_tokens=False).input_ids\n",
    "    if len(tt) == 0: # if there is none, set the embedding with the <unk> token).\n",
    "        print(f'empty token \"{t}\"/{tokenizer.convert_tokens_to_ids(t)}')\n",
    "        tt = [tokenizer_old.unk_token_id]\n",
    "        model.model.shared.weight.data[tokenizer.convert_tokens_to_ids(t)] = model.model.shared.weight.data[tt]\n",
    "    # re-initialize each new vocab with the average of the embeddings of the old tokens that corresponded to the new token\n",
    "    # if the new token consist of several tokens in old tokenizer except '▁' & '<unk>' token\n",
    "    elif (len(tt) > 1) and (tt != tokenizer.convert_tokens_to_ids(['▁', '<unk>'])):\n",
    "        model.model.shared.weight.data[tokenizer.convert_tokens_to_ids(t)] = model.model.shared.weight.data[tt].mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81f0ee",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e02bf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./nllb_zh_tr_new/tokenizer_config.json',\n",
       " './nllb_zh_tr_new/special_tokens_map.json',\n",
       " './nllb_zh_tr_new/sentencepiece.bpe.model',\n",
       " './nllb_zh_tr_new/added_tokens.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = './nllb_zh_tr_new'\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
